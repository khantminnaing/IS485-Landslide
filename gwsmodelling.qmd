---
title: "Parametric Testing"
author:
  - name: Khant Min Naing
  - name: Ann Mei Yi Victoria Grace
date: 01-07-2024 
date-modified: "last-modified"
categories:
  - R
  - sf
  - gwmodel
output:
  distill::distill_article:
    code_folding: false
    toc: true
    self_contained: false
---

To develop a landslide susceptibility methodology framework, we will explore and calibrate different statistical and machine learning models.

## 1.0 Import Packages

```{r}
pacman::p_load(sp, sf, st, spdep, raster, spatstat, tmap, devtools,vtable,ggplot2,egg, corrplot, patchwork, ggstats, ggstatsplot, GWmodel, tidyverse, gtsummary,vtable, sjPlot, sjmisc, sjlabelled, tableHTML, olsrr, car, blorr,ISLR, klaR)
```

## 2.0 Import Data

```{r}
valtellina <- read_sf(dsn = "./data/vector", layer = "valtellina")
train_grids_v4 <- read.csv("~/IS485-Landslide/data/aspatial/train_grid_v4.csv")
```

```{r}
train_grid_v4.sf <- st_as_sf(train_grids_v4,
                            coords = c("X", "Y"))
train_grid_v4.sf <- st_set_crs(train_grid_v4.sf, 32632) 
```

## 3.0 Exploratory Spatial Data Analysis (ESDA) 

To calculate the summary statistics of `landslide_train` data frame, we use `st()`.

```{r}
st(train_grids_v4)
```

Next, we will create atrellis plot by using `ggarrange()` of [**ggpubr**](https://cran.r-project.org/web/packages/ggpubr/) package. In this way, we can see the distribution plots of different parameters at the same time.4.1 Correlation Matrix Using Corrplot

Before building a logistic regression model, it is important to ensure that the indepdent variables used are **not highly correlated** to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as **multicollinearity** in statistics.

Correlation matrix is commonly used to visualise the relationships between the independent variables. In this section, the [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) package will be used to display the correlation matrix of the independent variables in *condo_resale* data frame.

```{r}
#| fig-width: 10

corrplot(cor(train_grids_v4[, 6:29]), diag = FALSE, order = "AOE",
         col=colorRampPalette(c("#50a8b4","#e4c838","#be804f"))(10),
         tl.pos = "td", tl.cex = 0.5,tl.col = "black", number.cex = 0.5, method = "number", type = "upper")

corrplot(cor(train_grids_v4[,6:29]), diag = FALSE, order = "AOE",
         col=colorRampPalette(c("#50a8b4","#ffffdd","#be804f"))(10),
         tl.pos = "td", tl.cex = 0.5,tl.col = "black", number.cex = 0.5, method = "ellipse", type = "upper")
```

Matrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named "AOE", "FPC", "hclust", "alphabet". In the code chunk above, AOE order is used. It orders the variables by using the *angular order of the eigenvectors* method suggested by [Michael Friendly](https://www.datavis.ca/papers/corrgram.pdf).

### 4.2 Correlation Matrix Using ggstats

```{r}
#| fig-width: 10

set.seed(123)
## producing the correlation matrix
ggcorrmat(
  data = train_grids_v4[, 6:29],  
          matrix.type = "upper",
  type = "parametric",
  tr = 0.2,
  partial = FALSE,
  k = 2L,
  sig.level = 0.05,
  conf.level = 0.95,
  bf.prior = 0.707,
  ggcorrplot.args = list(
     tl.cex = 10,
     pch.cex = 5,
     lab_size = 3
  )) + ## modification outside `{ggstatsplot}` using `{ggplot2}` functions
  ggplot2::theme(
    axis.text.x = ggplot2::element_text(
      margin = ggplot2::margin(t = 0.15, r = 0.15, b = 0.15, l = 0.15, unit = "cm")
    )
  )
```

## Multiple Logistic Regression

We will fit a logistic regression model in order to predict the probability of a customer defaulting based on the average balance carried by the customer. The `glm` function fits generalized linear models, a class of models that includes logistic regression. The syntax of the `glm` function is similar to that of `lm`, except that we must pass the argument `family = binomial` in order to tell R to run a logistic regression rather than some other type of generalized linear model.

```{r}
landslide.lr <- glm(Landslide ~ Elevation + Slope_Angle + Aspect_North + Aspect_NorthEast + Aspect_East+Aspect_SouthEast+Aspect_South + Aspect_SouthWest +Aspect_West + Profile_Curvature +Plan_Curvature + Lithology_Metamorphic+Lithology_Sedimentary + Lithology_Plutonic+Lithology_Unconsolidated + Proximity_Settlement+Proximity_Stream+Proximity_Road+Proximity_Fault+Landuse_Vegetation+Precipitation+TWI+SPI+STI, family = "binomial", data = train_grids_v4)
```

```{r}
summary(landslide.lr)
```

```{r}
100*with(summary(landslide.lr), 1 - deviance/null.deviance)
confint(landslide.lr)
```

```{r}
#| eval: false
tbl_regression(landslide.lr, intercept = TRUE) %>%
  add_glance_source_note(
  include = c(AIC))
```

### 4.5 Calculating Adjusted Odd Ratios and Confidence Intervals

```{r}
#| eval: false
OR.CI <- cbind("AOR" = exp(coef(landslide.lr)),
                       exp(confint(landslide.lr)))[-1,]
round(OR.CI, 6)
```

```{r}
vif(landslide.lr)
```

### Stepwise Selection

For the initial/ first cut model, all the independent variables are put into the model. Our goal is to include a limited number of independent variables (5-15) which are all significant, without sacrificing too much on the model performance. The rationale behind not-including too many variables is that the model would be over fitted and would become unstable when tested on the validation sample. The variable reduction is done using forward or backward or stepwise variable selection procedures. We will use `blr_step_aic_both()` to shortlist predictors for our model.

```{r}
blr_step_aic_both(landslide.lr)
```

```{r}
landslide.lr %>%
  blr_step_aic_both() %>%
  plot()
```

### Model Update

```{r}
landslide.lr_modified <- glm(Landslide ~ Slope_Angle + Aspect_SouthWest + Aspect_South + Profile_Curvature +Plan_Curvature +Lithology_Unconsolidated+Proximity_Stream+Landuse_Vegetation+TWI, family = "binomial", data = train_grids_v4)
```

```{r}
summary(landslide.lr_modified)
vif(landslide.lr_modified)

```

## Model Fit Statistics

Model fit statistics are available to assess how well the model fits the data and to compare two different models.The output includes likelihood ratio test, AIC, BIC and a host of pseudo r-squared measures. You can read more about pseudo r-squared at <https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/>.

```{r}
blr_model_fit_stats(landslide.lr)
```

Compared with the basic model,

```{r}
blr_model_fit_stats(landslide.lr)
```

## Model Validation

```{r}
blr_confusion_matrix(landslide.lr, cutoff = 0.5)
```

### Hosmer Lemeshow Test

Hosmer and Lemeshow developed a goodness-of-fit test for logistic regression models with binary responses. The test involves dividing the data into approximately ten groups of roughly equal size based on the percentiles of the estimated probabilities. The observations are sorted in increasing order of their estimated probability of having an even outcome. The discrepancies between the observed and expected number of observations in these groups are summarized by the Pearson chi-square statistic, which is then compared to chi-square distribution with t degrees of freedom, where t is the number of groups minus 2. Lower values of Goodness-of-fit are preferred.

```{r}
blr_test_hosmer_lemeshow(landslide.lr_modified)
```

### ROC Curve

ROC curve is a graphical representation of the validity of cut-offs for a logistic regression model. The ROC curve is plotted using the sensitivity and specificity for all possible cut-offs, i.e., all the probability scores. The graph is plotted using sensitivity on the y-axis and 1-specificity on the x-axis. Any point on the ROC curve represents a sensitivity X (1-specificity) measure corresponding to a cut-off. The area under the ROC curve is used as a validation measure for the model -- the bigger the area the better is the model.

```{r}
landslide.lr_modified%>%
    blr_gains_table() %>%
  blr_roc_curve()
```

#### Influence Diagnostics

```{r}
blr_plot_diag_influence(landslide.lr_modified)
```

#### Fitted Values Diagnostics

```{r}
blr_plot_diag_fit(landslide.lr_modified)
```

## Weight of Evidence

```{r}
library("Information")

IV <- create_infotables(data=train_grids_v4[, 5:29],
                        valid=train_grids_v4[, 5:29],
                        y="Landslide")
kable(IV$Summary, row.names=FALSE)
```


### **Spatial Interpolation**

```{r}
valtellina_boundary <- st_union(valtellina) %>% st_sf()
```

```{r}
#| eval: false
library(sf)
library(terra)
grid <- terra::rast(valtellina_boundary, nrows = 10000, ncols = 15000)
xy <- terra::xyFromCell(grid, 1:ncell(grid))
coop <- st_as_sf(as.data.frame(xy), coords = c("x", "y"),
              crs = st_crs(valtellina_boundary))
coop <- st_filter(coop, valtellina_boundary)
qtm(coop)
```

```{r}
#| eval: false
# Voronoi
v <- terra::voronoi(x = terra::vect(train.res.sf), bnd = valtellina_boundary)
plot(v)
points(vect(train.res.sf), cex = 0.5)

# Prediction
v <- st_as_sf(v)
tm_shape(v) +
  tm_fill(col = "MLR_RES", palette = "viridis")+
  tm_layout(
    legend.outside=TRUE
  )
```

```{r}
#| eval: false
resp <- st_intersection(v, coop)
resp$pred <- resp$MLR_RES

pred_mean <- terra::rasterize(resp, grid, field = "pred", fun = "mean")
tm_shape(pred_mean) + 
  tm_raster(palette = "plasma")+
  tm_layout(
    legend.outside=TRUE
  )
```

### IDW: Inverse Distance Weighting

In the IDW method, values at unsampled locations are estimated as the weighted average of values from the rest of locations with weights inversely proportional to the distance between the unsampled and the sampled locations.

We can apply the IDW method with the [`gstat()`](https://rdrr.io/pkg/gstat/man/gstat.html) function of **gstat** and the following arguments:

-   `formula`: `vble ~ 1` to have an intercept only model,

-   `nmax`: number of neighbors is set equal to the total number of locations,

-   `idp`: inverse distance power is set to `idp = 1` to have weights with β=1�=1.

Then, we use the [`predict()`](https://rdrr.io/pkg/terra/man/predict.html) function to obtain the predictions and **tmap** to show the results

```{r}
#| eval: false
library(gstat)
res <- gstat(formula = MLR_RES ~ 1, locations = train.res.sf,
             nmax = nrow(train.res.sf), # use all the neighbors locations
             set = list(idp = 1)) # beta = 1 

resp <- predict(res, coop)
resp$x <- st_coordinates(resp)[,1]
resp$y <- st_coordinates(resp)[,2]
resp$pred <- resp$var1.pred

pred <- terra::rasterize(resp, grid, field = "pred", fun = "mean")

```

```{r}
#| eval: false
tm_shape(pred) + 
  tm_raster(palette = "viridis")+
  tm_layout(
    legend.outside=TRUE
  )
```
